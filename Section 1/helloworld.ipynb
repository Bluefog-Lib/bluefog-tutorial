{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BlueFog with Jupyter Notebook\n",
    "\n",
    "The **goal** of this ipython notebook is to show you how you can run BlueFog code in ipython notebook environment.\n",
    "\n",
    "As you know, the BlueFog framework is aimed to run decentralized algorithm over a cluster of processes or machines.\n",
    "Meanwhile, jupyter notebook is an execellent interactive frontend talking with single IPython process. The contradiction between a single frontend process and a cluster of backend processescan be completely circumvented, fortunately. The main idea is using the jupyter notebook as the driver to inform how the cluster will execute the BlueFog code. On one hand, the execution of BlueFog still runs over multiple processes so that you can enjoy the scalability and parallelism brought by BlueFog. On the other hand, you (the programmer) can interactively execute with BlueFog and monitor and control the code in the single place.\n",
    "\n",
    "## Prerequisites: \n",
    "To begin with, make sure that you have installed OpenMPI and BlueFog in your system first.\n",
    "Besides these, you also need to install [ipyparallel](https://ipyparallel.readthedocs.io/en/latest/index.html) package for managering the parallel computation.\n",
    "Usually, you can install it through `pip install jupyter ipyparallel`.\n",
    "\n",
    "## Steps to start Bluefog with Jupyter Notebook\n",
    "\n",
    "- **Step 1: Start jupyter notebook**\n",
    "\n",
    "```shell\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "- **Step 2A: Start the `ibfrun` (interactive bluefog run) in a single machine**\n",
    "\n",
    "```shell\n",
    "ibfrun start -np 4\n",
    "```\n",
    "where `np` means the number of process. In this example command, it will start 4 processes. You are free to choose any number between 1 and the number of CPU processes in you machine. \n",
    "\n",
    "- **Step 2B: Start the `ibfrun` in multiple machines**\n",
    "\n",
    "```shell\n",
    "ibfrun start -np 16 -H machine-a:4,machine-b:4,machine-c:4,machine-d:4\n",
    "```\n",
    "where `machine-a` to `machine-d` are the machine name or the address of the machine and `:4` after the machine name means on that machine launch 4 processes. Before you run this command, please make sure that all the machines is able to be login through `ssh` **without** typing the password.\n",
    "\n",
    "That's it!\n",
    "\n",
    "**Other remarks**\n",
    "- Please make sure the jupyter notebook command and the ibfrun command are under the same machine.\n",
    "- If you want to communicate over GPUs through NCCL, the number of process in each machine should not exceed the number GPUs instead of CPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:23.697595Z",
     "start_time": "2021-01-16T06:50:22.766611Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect the Jupyter Notebook to the workers\n",
    "\n",
    "The first step is to let the jupyter notebook connect to the workers (cluster). To achieve that, import the ipypyarallel library since we rely on it to manage the workers, then connect it through `ipp.Client(profile=\"bluefog\")` as shown in the following cell.\n",
    "\n",
    "`rc` (remote client) class is the key that you will use to send the code from this notebook (driver) to the workers. Remote client is called as such because it talks with the workers, which may not be on the local host.\n",
    "To check if the notebook has connected to the worker succesfully, you can look the ID of rc through `rc.ids`.\n",
    "Depending on how many processes you start, the length of rc will be the same number as the number of processes you start the `ibfrun`, i.e., the number after `-np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:24.460204Z",
     "start_time": "2021-01-16T06:50:24.258809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile=\"bluefog\")\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you saw the `ids` is an empty list, this means ipyparallel cannot connect it with the workers. You can stop the `ibfrun` by `ctrl+c`. Kill all remainning `ipcontroller` or `mpi` processes through `ps` if necessary. Then, try to rerun the `ibfrun` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Parallel execution through `px`\n",
    "\n",
    "`px` -- parallel execution is a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) in IPython environment similar as `%matplotlib` or `%%timeit`. If you are not familar with magic command, you can think it as a special command in IPython that will change the behavior of IPython interpreting the following python code. \n",
    "\n",
    "Same as other magic commands, `%px` is for one line and `%%px` is for whole cell. For example, \n",
    "`%px print(\"Hello world\")` will put `print(\"Hello world\")` to run under the `px` environment. And, if you put the `%%px` at the top of one cell, the rest of code in that cell will be run under the `px` environment.\n",
    "\n",
    "The code under the `px` env will be executed in each worker in parallel instead of running in this notebook.\n",
    "It is important to remember that each worker is one seperated IPython environment. The following is a hello-world example to work with bluefog in jupyter notebook, which is already sufficient for most  usage cases. You can find more advanced usage of `px` in the [document page](https://ipyparallel.readthedocs.io/en/latest/magics.html).\n",
    "\n",
    "\n",
    "All BlueFog code should start with `bf.init()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:27.337955Z",
     "start_time": "2021-01-16T06:50:25.579567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Hello, I am 0 among 4 processes\n",
      "[stdout:1] Hello, I am 1 among 4 processes\n",
      "[stdout:2] Hello, I am 3 among 4 processes\n",
      "[stdout:3] Hello, I am 2 among 4 processes\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import bluefog.torch as bf\n",
    "\n",
    "bf.init()\n",
    "message = f\"Hello, I am {bf.rank()} among {bf.size()} processes\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `Hello, I am {x} among {np} processes` repeated `np` time where `x` ranges from 0 to np-1. This `x` is called rank. Note the code is the same while the outputs from the cluster of processes are different. This is the typical usage of BlueFog code, which belongs to the Single Instruction, Mutliple Data [SIMD](https://en.wikipedia.org/wiki/SIMD) pattern.\n",
    "\n",
    "You can also use `%px` for one line. A typical usage of `%px` is mixing the control flow code of notebook and the execution code in the workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:27.504324Z",
     "start_time": "2021-01-16T06:50:27.343746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "[stdout:0] Hello, I am 0 among 4 processes\n",
      "[stdout:1] Hello, I am 1 among 4 processes\n",
      "[stdout:2] Hello, I am 3 among 4 processes\n",
      "[stdout:3] Hello, I am 2 among 4 processes\n",
      "==========================================\n",
      "[stdout:0] Hello, I am 0 among 4 processes\n",
      "[stdout:1] Hello, I am 1 among 4 processes\n",
      "[stdout:2] Hello, I am 3 among 4 processes\n",
      "[stdout:3] Hello, I am 2 among 4 processes\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 42)\n",
    "for _ in range(2):\n",
    "    %px print(message)\n",
    "    print(\"=\" * 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each worker is a IPython environment, it will remember the context of execution. Hence, the `message` variable is still the same `message` variable created in previous cell and `bf.init()` just needs to be called once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving the data between notebook and workers\n",
    "\n",
    "Only executing the BlueFog code in multiple workers through notebook is not interesting enough. Because all running results only remain in the workers. The notebook does not know those results by default. You can try to run `print(message)` in the cell without `px`, you will encounter the `NameError`, i.e., that variable is not defined under the notebook.\n",
    "\n",
    "Fortunately, we can easily extract or pass the data or objects between notebook and workers.\n",
    "The key componenet is [Direct View class](https://ipyparallel.readthedocs.io/en/latest/direct.html#) through `dview=rc[:]`. Using `dview.push()` and `dview.pull()`, they transmit the data between workers and notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:33.853374Z",
     "start_time": "2021-01-16T06:50:33.808900Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import torch\n",
    "x = torch.FloatTensor([bf.rank()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each worker creates a tensor `x` containing the 1x1 value equal to its corresponding rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:34.988406Z",
     "start_time": "2021-01-16T06:50:34.889322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview = rc[:] # A DirectView of all engines\n",
    "dview.block=True\n",
    "x_at_worker0 = dview.pull('x', targets=0)\n",
    "x_at_worker0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the code above did is that notebook pulls the value `x` of worker 0 and stores that as `x_at_worker0` in the notebook. Note the worker 0 may not be the worker that has rank 0. Hence, the value of `x_at_worker0` may not be zero. You also pull the results from all workers or part of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:35.972698Z",
     "start_time": "2021-01-16T06:50:35.947314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.]), tensor([3.])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_at_1_and_2 = dview.pull('x', targets=[1, 2])\n",
    "xs_at_1_and_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:36.279305Z",
     "start_time": "2021-01-16T06:50:36.249560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.]), tensor([1.]), tensor([3.]), tensor([2.])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_all = dview.pull('x')\n",
    "xs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push the data from the notebook to all workers through `dview.push(arg)`, where the `arg` is a dictionary that map from the variable name to variable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:37.155838Z",
     "start_time": "2021-01-16T06:50:37.138090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the data into all workers\n",
    "dview.push({'seed': 12345})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:50:37.645339Z",
     "start_time": "2021-01-16T06:50:37.612124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] I received seed as value:  12345\n",
      "[stdout:1] I received seed as value:  12345\n",
      "[stdout:2] I received seed as value:  12345\n",
      "[stdout:3] I received seed as value:  12345\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(\"I received seed as value: \", seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Working with multiple GPUs\n",
    "\n",
    "If you have multiple GPUs, it is typical that each worker is pinned to one device.\n",
    "Then, you can let the tensor wihin each worker set on different GPUs as shown in following code.\n",
    "\n",
    "*Note: If you want to pin multiple workers in one device, you cannot use NCCL as communication backend.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] tensor([0.], device='cuda:0')\n",
      "[stdout:1] tensor([1.], device='cuda:1')\n",
      "[stdout:2] tensor([3.], device='cuda:3')\n",
      "[stdout:3] tensor([2.], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(bf.local_rank())\n",
    "    x = torch.FloatTensor([bf.rank()]).cuda()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrupt the hanged worker process\n",
    "\n",
    "It is quite common in the code dealing with the multiple machine communication that some unexpected bugs or deadlock cause the worker process(es) hang forever. Recall the worker processes are inside a seperated environment from the notebook. Hence, the 'interrupt the kernel' button, i.e. black square box, in the toolbar of the notebook interrupts the code of execution in notebook but the worker processes are still running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `sleep()` function to simulate that situation. If you run the following code and click the interrupt button afterwards, you will be able to execute other cells but the workers still continue to sleep, which will block the running of other requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:36:53.518983Z",
     "start_time": "2021-01-16T06:36:50.581376Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import time\n",
    "time.sleep(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:46:17.603676Z",
     "start_time": "2021-01-16T06:46:17.545780Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# You will not see the output immediately, even if you stop the previous cell.\n",
    "# It is because the workers are still sleeping. The interruption in notebook\n",
    "# does not interrupt the workers.\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, BlueFog provides a utility function `interrupt_hanged_processes()`. After running this function, the previous `sleep()` function will be interrupted so that you should be able to execute other functions afterwards. Meanwhile, the context of IPython remains.\n",
    "\n",
    "*Note: `interrupt_hanged_processes()` is supported under the localhost mode only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T07:30:31.620123Z",
     "start_time": "2021-01-17T07:30:30.737987Z"
    }
   },
   "outputs": [],
   "source": [
    "from bluefog.run.interactive_run import interrupt_hanged_processes\n",
    "interrupt_hanged_processes(profile=\"bluefog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T06:46:41.734611Z",
     "start_time": "2021-01-16T06:46:41.677432Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Now you should see the output immediately.\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens after you run `ibfrun`\n",
    "\n",
    "If you are curious about what happened after running the `ibfrun` command, the following figure should give you a good overview.\n",
    "\n",
    "<img src=\"./ibfrun.png\" alt=\"ibfrun\" width=\"780\"/>\n",
    "\n",
    "When you start the jupyter notebook, you will create a IPython client, which will keep recieving and executing the command from the notebook. `ibfrun` will create a controller and a bunch of workers (They are actually IPython Kernels) under the MPI environment. The controller knows how to communicate with the IPython client and the workers. When you execute the code under `px` environment, the following procedure happens:\n",
    "\n",
    "1. The notebook sends the user code back to the IPython client.\n",
    "2. The IPython client notices this code is needed to be run under `px` so it forwards the code to the controller.\n",
    "3. The controller forwards the code to the worker according to `--targets`. By default, it forwards to all workers.\n",
    "4. The workers accept the command and execute it in parallel. If there is data exchange among the processes, the communication will be performed through the MPI or NCCL.\n",
    "5. After all workers finish executing the code, the jupyter notebook will be notified that the execution is done.\n",
    "\n",
    "\n",
    "As for the `push` and `pull` commands in direct view, their execution procedures are similar as the `px` data flow.\n",
    "\n",
    "Last, notice that `ibfrun` is just a thin wrapper for convenience over the command provided by the `ipyparallel`. If you have some speical requirement or you have problem of executing `ibfrun`, you can use the `ipcontroller` or `ipengin` command directly. Check [ipyparallel document](https://ipyparallel.readthedocs.io/en/latest/process.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suspend BlueFog\n",
    "Bluefog launches a background process, which may consume CPUs a lot.\n",
    "even if you don't use it actively. You can suspend it through `bf.suspend()`.\n",
    "Note after it is suspended, you are no longer able to run BlueFog communication functions.\n",
    "You can continue the usage through `bf.resume()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T01:58:05.610701Z",
     "start_time": "2021-01-02T01:58:05.579153Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "bf.suspend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T01:58:06.154380Z",
     "start_time": "2021-01-02T01:58:06.130947Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "bf.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

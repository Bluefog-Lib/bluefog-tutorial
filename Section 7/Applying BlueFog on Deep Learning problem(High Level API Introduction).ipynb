{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "409d27520da0"
   },
   "source": [
    "# Applying BlueFog on Deep Learning problem(High Level API Introduction)\n",
    "\n",
    "All previous sections we focused on the low-level API in the BlueFog, which is great for flexible algorithm design and research. Give a quick summary here:\n",
    "\n",
    "- Basic static topology property and its manipulation\n",
    "- Collective communication such as broadcast and allreduce\n",
    "- Topology based Neighborhood communication such as neighbor_allreduce.\n",
    "- Blocking versus non-blocking operation\n",
    "- Dynamic topology and its corresponding\n",
    "- Asynchronous operation through window object\n",
    "- Examine numerous algorithms and their performance under different scenarios\n",
    "- etc.\n",
    "\n",
    "However, it can be boilerplate if you want to apply one certain algorithm on different tasks. Further, it is also tricky to write a efficiency code combining above mentioned concepts. This becomes even worse in the deep learning problem. Backpropagation property of neural network makes that the gradient can be efficiently calculated. Backpropagation also implied that the gradient is calculated (approximately) in layer-by-layer style, in contrast to one global stochastic (sub-)gradient we encountered in the optimization. Further, this layer-wise computation provides a great opportunity to overlap the communication and comptuation for minimizing the trainning time, which is a crucial. Clearly, writing the code to address these concern correctly and efficient is not easy. Hence, BlueFog further provides the high level APIs, which can be directly applied on the `torch.Optimizer` directly. \n",
    "\n",
    "In this section, we will focus in applying high level APIs of BlueFog on Deep Learning problem, mainly the decentralized trainning task.\n",
    "Before we demystify how we implement the High-Level API in BlueFog, let's see the example of using them to write distributed trainning of ResNet-18 model over CIFAR-10 easily. (*Note, although this example is relative small, it still can be time-consuming and drain tons of computation resources if you want to train them on CPUs.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "16e02cc90976"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import ipyparallel as ipp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "579523c07451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = ipp.Client(profile=\"bluefog\")\n",
    "rc.block=True\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5282f167c4f0"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import torch\n",
    "import bluefog.torch as bf\n",
    "\n",
    "bf.init()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Applying BlueFog on Deep Learning problem(High Level API Introduction).ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

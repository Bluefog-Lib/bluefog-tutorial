{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "409d27520da0"
   },
   "source": [
    "# Applying BlueFog on Deep Learning problem(High Level API Introduction)\n",
    "\n",
    "All previous sections we focused on the low-level API in the BlueFog, which is great for flexible algorithm design and research. Give a quick summary here:\n",
    "\n",
    "- Basic static topology property and its manipulation\n",
    "- Collective communication such as broadcast and allreduce\n",
    "- Topology based Neighborhood communication such as neighbor_allreduce.\n",
    "- Blocking versus non-blocking operation\n",
    "- Dynamic topology and its corresponding\n",
    "- Asynchronous operation through window object\n",
    "- Examine numerous algorithms and their performance under different scenarios\n",
    "- etc.\n",
    "\n",
    "However, it can be boilerplate if you want to apply one certain algorithm on different tasks. Further, it is also tricky to write a efficiency code combining above mentioned concepts. This becomes even worse in the deep learning problem. Backpropagation property of neural network makes that the gradient can be efficiently calculated. Backpropagation also implied that the gradient is calculated (approximately) in layer-by-layer style, in contrast to one global stochastic (sub-)gradient we encountered in the optimization. Further, this layer-wise computation provides a great opportunity to overlap the communication and comptuation for minimizing the trainning time, which is a crucial. Clearly, writing the code to address these concern correctly and efficient is not easy. Hence, BlueFog further provides the high level APIs, which can be directly applied on the `torch.Optimizer` directly. \n",
    "\n",
    "In this section, we will focus in applying high level APIs of BlueFog on Deep Learning problem, mainly the decentralized trainning task.\n",
    "Before we demystify how we implement the High-Level API in BlueFog, let's see the example of using them to write distributed trainning of ResNet-18 model over CIFAR-10 easily. (*Note, although this example is relative small, it still can be time-consuming and drain tons of computation resources if you want to train them on CPUs.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16e02cc90976"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ipyparallel as ipp\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "579523c07451"
   },
   "outputs": [],
   "source": [
    "rc = ipp.Client(profile=\"bluefog\")\n",
    "rc.block=True\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b287fd7db903"
   },
   "outputs": [],
   "source": [
    "# Down the CIFAR10 Dataset if not available\n",
    "# Since the dataset is smaller enough, we just load it in-memory.\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    os.path.join(cwd_folder_loc, \"..\", \"data\"),\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6db4fe3f6cfe"
   },
   "outputs": [],
   "source": [
    "# Distribute the data into each worker.\n",
    "# Note we push the full dataset into each worker is just for simplicity.\n",
    "# Each worker only read the partial of dataset later.\n",
    "_ = rc[:].push({\"train_dataset\": train_dataset, \"val_dataset\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5282f167c4f0"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import torch\n",
    "import bluefog.torch as bf\n",
    "from bluefog.common import topology_util\n",
    "\n",
    "seed = 2021\n",
    "bf.init()\n",
    "torch.manual_seed(seed)\n",
    "run_on_cuda = torch.cuda.is_available()\n",
    "if run_on_cuda:\n",
    "    print(\"using cuda.\")\n",
    "    # Bluefog: pin GPU to local rank.\n",
    "    device_id = (bf.local_rank() if bf.nccl_built() else bf.local_rank() %\n",
    "                 torch.cuda.device_count())\n",
    "    torch.cuda.set_device(device_id)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4311e103526c"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Prepare the distributed loader for dataset.\n",
    "batch_size = 32\n",
    "val_batch_size = 1024\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if run_on_cuda else {}\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=bf.size(), rank=bf.rank())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler,\n",
    "                                           **kwargs)\n",
    "\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    val_dataset, num_replicas=bf.size(), rank=bf.rank())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=val_batch_size,\n",
    "                                         sampler=val_sampler,\n",
    "                                         **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cb62e20c52f"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "model = models.resnet18(num_classes=10)\n",
    "if run_on_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Scale learning rate by the number of GPUs.\n",
    "base_lr = 0.0125\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00005\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=(base_lr * bf.size()),\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db3803bdc39d"
   },
   "source": [
    "## Wrap the torch standard optimizer into BlueFog distributed one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0016e16b8e2e"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "atc_style = False\n",
    "base_dist_optimizer = (bf.DistributedAdaptThenCombineOptimizer if atc_style\n",
    "                       else bf.DistributedAdaptWithCombineOptimizer)\n",
    "optimizer = base_dist_optimizer(\n",
    "    optimizer,\n",
    "    model=model,\n",
    "    communication_type=bf.CommunicationType.neighbor_allreduce)\n",
    "\n",
    "# Bluefog: broadcast parameters & optimizer state.\n",
    "bf.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "bf.broadcast_optimizer_state(optimizer, root_rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "424fe7146cc1"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).cpu().float().mean()\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "        self.n = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += bf.allreduce(val.detach().cpu(), name=self.name)\n",
    "        self.n += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "\n",
    "dynamic_neighbor_allreduce_gen = topology_util.GetDynamicOnePeerSendRecvRanks(\n",
    "    bf.load_topology(), bf.rank())\n",
    "def dynamic_topology_update(epoch, batch_idx):\n",
    "    send_neighbors, recv_neighbors = next(dynamic_neighbor_allreduce_gen)\n",
    "    optimizer.send_neighbors = send_neighbors\n",
    "    optimizer.neighbor_weights = {\n",
    "        r: 1 / (len(recv_neighbors) + 1)\n",
    "        for r in recv_neighbors\n",
    "    }\n",
    "    optimizer.self_weight = 1 / (len(recv_neighbors) + 1)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(epoch, batch_idx):\n",
    "    if epoch < 5:  # warmup_epochs\n",
    "        epoch += float(batch_idx + 1) / len(train_loader)\n",
    "        lr_adj = 1.0 / bf.size() * (epoch *\n",
    "                                    (bf.size() - 1) / 5 + 1)\n",
    "    elif epoch < 30:\n",
    "        lr_adj = 1.0\n",
    "    elif epoch < 60:\n",
    "        lr_adj = 1e-1\n",
    "    elif epoch < 80:\n",
    "        lr_adj = 1e-2\n",
    "    else:\n",
    "        lr_adj = 1e-3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = (base_lr * bf.size() * lr_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e5522361fa3"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Check how to show tqdm correctly here???\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_loss = Metric(\"train_loss\")\n",
    "    train_accuracy = Metric(\"train_accuracy\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        adjust_learning_rate(epoch, batch_idx)\n",
    "        dynamic_topology_update(epoch, batch_idx)\n",
    "\n",
    "        if run_on_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        train_accuracy.update(accuracy(output, target))\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        train_loss.update(loss)\n",
    "        # Average gradients among sub-batches\n",
    "        loss.div_(math.ceil(float(len(data)) / batch_size))\n",
    "        loss.backward()\n",
    "        # Gradient is applied across all ranks\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = Metric(\"val_loss\")\n",
    "    val_accuracy = Metric(\"val_accuracy\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            if run_on_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "\n",
    "            val_loss.update(F.cross_entropy(output, target))\n",
    "            val_accuracy.update(accuracy(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91ea0e330ab3"
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    rc[:].push({\"epoch\": epochs})\n",
    "    %px train(epoch)\n",
    "    %px validate(epoch)\n",
    "    # TODO pull the accuracy and result out from worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51577e0b594e"
   },
   "source": [
    "# Demystify the BlueFog DistributedAdaptThenCombineOptimizer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Applying BlueFog on Deep Learning problem(High Level API Introduction).ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

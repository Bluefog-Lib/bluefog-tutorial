{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "409d27520da0"
   },
   "source": [
    "# Applying BlueFog on Deep Learning problem(High Level API Introduction)\n",
    "\n",
    "All previous sections we focused on the low-level API in the BlueFog, which is great for flexible algorithm design and research. Give a quick summary here:\n",
    "\n",
    "- Basic static topology propertis and how to manipulate it\n",
    "- Collective communication such as broadcast and allreduce\n",
    "- Topology based Neighborhood communication such as neighbor_allreduce.\n",
    "- Blocking versus non-blocking operation\n",
    "- Dynamic topology and its corresponding\n",
    "- Asynchronous operation through window object\n",
    "- Several decentralized algorithms and their performance under different scenarios\n",
    "- etc.\n",
    "\n",
    "However, it can be boilerplate if you want to apply one certain algorithm on different tasks. Further, it is also tricky to write a efficiency code combining above mentioned concepts. This becomes even worse in the deep learning problem. Backpropagation property of neural network makes that the gradient can be efficiently calculated. Backpropagation also implied that the gradient is calculated (approximately) in layer-by-layer style, in contrast to one global stochastic (sub-)gradient we encountered in the optimization. Further, this layer-wise computation provides a great opportunity to overlap the communication and comptuation for minimizing the trainning time, which is a crucial. Clearly, writing the code to address these concern correctly and efficient is not easy. Hence, BlueFog further provides the high level APIs, which can be directly applied on the `torch.Optimizer` directly. \n",
    "\n",
    "In this section, we will focus in applying high level APIs of BlueFog on Deep Learning problem, mainly the decentralized trainning task.\n",
    "Before we demystify how we implement the High-Level API in BlueFog, let's see the example of using them to write distributed trainning of ResNet-18 model over CIFAR-10 easily. (*Note, although this example is relative small, it still can be time-consuming and drain tons of computation resources if you want to train them on CPUs.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "16e02cc90976"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ipyparallel as ipp\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "579523c07451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = ipp.Client(profile=\"bluefog\")\n",
    "rc.block=True\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b287fd7db903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Down the CIFAR10 Dataset if not available\n",
    "# Since the dataset is smaller enough, we just load it in-memory.\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6db4fe3f6cfe"
   },
   "outputs": [],
   "source": [
    "# Distribute the data into each worker.\n",
    "# Note we push the full dataset into each worker is just for simplicity.\n",
    "# Each worker only read the partial of dataset later.\n",
    "_ = rc[:].push({\"train_dataset\": train_dataset, \"val_dataset\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5282f167c4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] using cuda.\n",
      "[stdout:1] using cuda.\n",
      "[stdout:2] using cuda.\n",
      "[stdout:3] using cuda.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import math\n",
    "import torch\n",
    "import bluefog.torch as bf\n",
    "from bluefog.common import topology_util\n",
    "\n",
    "seed = 2021\n",
    "bf.init()\n",
    "torch.manual_seed(seed)\n",
    "run_on_cuda = torch.cuda.is_available()\n",
    "if run_on_cuda:\n",
    "    print(\"using cuda.\")\n",
    "    # Bluefog: pin GPU to local rank.\n",
    "    device_id = (bf.local_rank() if bf.nccl_built() else bf.local_rank() %\n",
    "                 torch.cuda.device_count())\n",
    "    torch.cuda.set_device(device_id)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4311e103526c"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Prepare the distributed loader for dataset.\n",
    "batch_size = 32\n",
    "val_batch_size = 1024\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if run_on_cuda else {}\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=bf.size(), rank=bf.rank())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler,\n",
    "                                           **kwargs)\n",
    "\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    val_dataset, num_replicas=bf.size(), rank=bf.rank())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=val_batch_size,\n",
    "                                         sampler=val_sampler,\n",
    "                                         **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6cb62e20c52f"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "from torchvision import models\n",
    "model = models.resnet18(num_classes=10)\n",
    "if run_on_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Scale learning rate by the number of GPUs.\n",
    "base_lr = 0.0125\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00005\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=(base_lr * bf.size()),\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db3803bdc39d"
   },
   "source": [
    "## Wrap the torch standard optimizer into BlueFog distributed one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0016e16b8e2e"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "atc_style = False\n",
    "base_dist_optimizer = (bf.DistributedAdaptThenCombineOptimizer if atc_style\n",
    "                       else bf.DistributedAdaptWithCombineOptimizer)\n",
    "optimizer = base_dist_optimizer(\n",
    "    optimizer,\n",
    "    model=model,\n",
    "    communication_type=bf.CommunicationType.neighbor_allreduce)\n",
    "\n",
    "# Bluefog: broadcast parameters & optimizer state.\n",
    "bf.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "bf.broadcast_optimizer_state(optimizer, root_rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "424fe7146cc1"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).cpu().float().mean()\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "        self.n = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += bf.allreduce(val.detach().cpu(), name=self.name)\n",
    "        self.n += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "\n",
    "dynamic_neighbor_allreduce_gen = topology_util.GetDynamicOnePeerSendRecvRanks(\n",
    "    bf.load_topology(), bf.rank())\n",
    "def dynamic_topology_update(epoch, batch_idx):\n",
    "    send_neighbors, recv_neighbors = next(dynamic_neighbor_allreduce_gen)\n",
    "    optimizer.send_neighbors = send_neighbors\n",
    "    optimizer.neighbor_weights = {\n",
    "        r: 1 / (len(recv_neighbors) + 1)\n",
    "        for r in recv_neighbors\n",
    "    }\n",
    "    optimizer.self_weight = 1 / (len(recv_neighbors) + 1)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(epoch, batch_idx):\n",
    "    if epoch < 5:  # warmup_epochs\n",
    "        epoch += float(batch_idx + 1) / len(train_loader)\n",
    "        lr_adj = 1.0 / bf.size() * (epoch *\n",
    "                                    (bf.size() - 1) / 5 + 1)\n",
    "    elif epoch < 30:\n",
    "        lr_adj = 1.0\n",
    "    elif epoch < 60:\n",
    "        lr_adj = 1e-1\n",
    "    elif epoch < 80:\n",
    "        lr_adj = 1e-2\n",
    "    else:\n",
    "        lr_adj = 1e-3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = (base_lr * bf.size() * lr_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0e5522361fa3"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Check how to show tqdm correctly here???\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_loss = Metric(\"train_loss\")\n",
    "    train_accuracy = Metric(\"train_accuracy\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        adjust_learning_rate(epoch, batch_idx)\n",
    "        dynamic_topology_update(epoch, batch_idx)\n",
    "\n",
    "        if run_on_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        train_accuracy.update(accuracy(output, target))\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        train_loss.update(loss)\n",
    "        # Average gradients among sub-batches\n",
    "        loss.div_(math.ceil(float(len(data)) / batch_size))\n",
    "        loss.backward()\n",
    "        # Gradient is applied across all ranks\n",
    "        optimizer.step()\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = Metric(\"val_loss\")\n",
    "    val_accuracy = Metric(\"val_accuracy\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            if run_on_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "\n",
    "            val_loss.update(torch.nn.functional.cross_entropy(output, target))\n",
    "            val_accuracy.update(accuracy(output, target))\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "91ea0e330ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  1.7248756885528564 train_accuracy 41.727542877197266\n",
      "val_loss:  1.4801629781723022 val_accuracy 53.830915689468384\n",
      "train_loss:  1.206300973892212 train_accuracy 58.22610259056091\n",
      "val_loss:  1.2425800561904907 val_accuracy 61.185747385025024\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    rc[:].push({\"epoch\": epochs}, block=True)\n",
    "    %px train_loss, train_accuracy = train(epoch)\n",
    "    %px val_loss, val_accuracy = validate(epoch)\n",
    "    # TODO pull the accuracy and result out from worker.\n",
    "    \n",
    "    train_loss, train_accuracy = rc[0].pull(\n",
    "        [\"train_loss\",\"train_accuracy\"], block=True)\n",
    "    val_loss, val_accuracy = rc[0].pull(\n",
    "        [\"val_loss\", \"val_accuracy\"], block=True)\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    print(\"train_loss: \", train_loss.avg.item(),\n",
    "          \"train_accuracy\", 100.0 * train_accuracy.avg.item())\n",
    "    print(\"val_loss: \", val_loss.avg.item(),\n",
    "          \"val_accuracy\", 100.0 * val_accuracy.avg.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51577e0b594e"
   },
   "source": [
    "# Demystify the BlueFog High Level APIs\n",
    "\n",
    "Here we show more details about two common BlueFog high level APIs -- `DistributedAdaptWithCombineOptimizer` and `DistributedAdaptThenCombineOptimizer`. They are corresponding to ATC and AWC algorithm we introduced before. After understanding how these works, you should be able to quickly grasp main idea of other high level APIs or even build your own high level APIs with different decentralized algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "800ee87c54bb"
   },
   "source": [
    "From the algorithm part, there is nothing new happened under the these two distributed optimizer. But we update our notation slightly since in neural network it is common to denote the gradient layer by layer:\n",
    "\n",
    "\\begin{align}\n",
    "    grad_{l}(w_{k,i}) \n",
    "     =& \\frac{\\partial f_k(w_{k,i})}{\\partial w_{k,i}[l]} \\\\\n",
    "     =& \\frac{\\partial f_k(w_{k,i})}{\\partial w_{k,i}[l+1]}\\frac{\\partial w_{k,i}[l+1]}{\\partial w_{k,i}[l]} \\\\\n",
    "     =& grad_{l+1}(w_{k,i}) \\frac{\\partial w_{k,i}[l+1]}{\\partial w_{k,i}[l]}\n",
    "\\end{align}\n",
    "where we use $l$ for $l-$ layer and $[l]$ means the parameter related to layer $l$. In this notation, it is closer to reflect how torch works. Except that, the AWC and ATC are applied in the same way. However, in the engineering part,\n",
    "it is not the same as we did before. The main difference is that we no longer wait for the gradient computation of whole model finished to start the communication. Instead, we try to maximize the communication and computation through layer-wise style. \n",
    "\n",
    "\n",
    "First, let's review the typical deep learning training under Torch framework:\n",
    "```python\n",
    "loss = metric(model(data), target)\n",
    "loss.backward()  <--- # it starts to compute the gradient\n",
    "optimizer.step() <--- # it applies the computed gradient on the weights\n",
    "```\n",
    "The `backwards()` function will inversely traverse the neural networks and compute the gradients of each components in the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ddee22c408a2"
   },
   "source": [
    "<img src=\"atc_awc_timeline.png\" alt=\"atc_awc_timeline.png\" width=\"650\"/>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Applying BlueFog on Deep Learning problem(High Level API Introduction).ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "409d27520da0"
   },
   "source": [
    "# Applying BlueFog on Deep Learning problem(High Level API Introduction)\n",
    "\n",
    "All previous sections we focused on the low-level API in the BlueFog, which is great for flexible algorithm design and research. Give a quick summary here:\n",
    "\n",
    "- Basic static topology propertis and how to manipulate it\n",
    "- Collective communication such as broadcast and allreduce\n",
    "- Topology based Neighborhood communication such as neighbor_allreduce.\n",
    "- Blocking versus non-blocking operation\n",
    "- Dynamic topology and its corresponding\n",
    "- Asynchronous operation through window object\n",
    "- Several decentralized algorithms and their performance under different scenarios\n",
    "- etc.\n",
    "\n",
    "However, it can be boilerplate if you want to apply one certain algorithm on different tasks. Further, it is also tricky to write a efficiency code combining above mentioned concepts. This becomes even worse in the deep learning problem. Backpropagation property of neural network makes that the gradient can be efficiently calculated. Backpropagation also implied that the gradient is calculated (approximately) in layer-by-layer style, in contrast to one global stochastic (sub-)gradient we encountered in the optimization. Further, this layer-wise computation provides a great opportunity to overlap the communication and comptuation for minimizing the trainning time, which is a crucial. Clearly, writing the code to address these concern correctly and efficient is not easy. Hence, BlueFog further provides the high level APIs, which can be directly applied on the `torch.Optimizer` directly. \n",
    "\n",
    "In this section, we will focus in applying high level APIs of BlueFog on Deep Learning problem, mainly the decentralized trainning task.\n",
    "Before we demystify how we implement the High-Level API in BlueFog, let's see the example of using them to write distributed trainning of ResNet-18 model over CIFAR-10 easily. (*Note, although this example is relative small, it still can be time-consuming and drain tons of computation resources if you want to train them on CPUs.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "16e02cc90976"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import ipyparallel as ipp\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "579523c07451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = ipp.Client(profile=\"bluefog\")\n",
    "rc.block = True\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc6185ac461f"
   },
   "source": [
    "## Prepared the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b287fd7db903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Down the CIFAR10 Dataset if not available\n",
    "# Since the dataset is smaller enough, we just load it in-memory.\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6db4fe3f6cfe"
   },
   "outputs": [],
   "source": [
    "# Distribute the data into each worker.\n",
    "# Note we push the full dataset into each worker is just for simplicity.\n",
    "# Each worker only read the partial of dataset later.\n",
    "_ = rc[:].push({\"train_dataset\": train_dataset, \"val_dataset\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5282f167c4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] using cuda.\n",
      "[stdout:1] using cuda.\n",
      "[stdout:2] using cuda.\n",
      "[stdout:3] using cuda.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import math\n",
    "import torch\n",
    "import bluefog.torch as bf\n",
    "from bluefog.common import topology_util\n",
    "\n",
    "seed = 2021\n",
    "bf.init()\n",
    "torch.manual_seed(seed)\n",
    "run_on_cuda = torch.cuda.is_available()\n",
    "if run_on_cuda:\n",
    "    print(\"using cuda.\")\n",
    "    # Bluefog: pin GPU to local rank.\n",
    "    device_id = (\n",
    "        bf.local_rank()\n",
    "        if bf.nccl_built()\n",
    "        else bf.local_rank() % torch.cuda.device_count()\n",
    "    )\n",
    "    torch.cuda.set_device(device_id)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "59190bd2f1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /home/kun/anaconda3/envs/bluefog/lib/python3.7/site-packages/torch/__init__.py\n",
      "[stdout:1] /home/kun/anaconda3/envs/bluefog/lib/python3.7/site-packages/torch/__init__.py\n",
      "[stdout:2] /home/kun/anaconda3/envs/bluefog/lib/python3.7/site-packages/torch/__init__.py\n",
      "[stdout:3] /home/kun/anaconda3/envs/bluefog/lib/python3.7/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4311e103526c"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Prepare the distributed loader for dataset.\n",
    "batch_size = 32\n",
    "val_batch_size = 1024\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if run_on_cuda else {}\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=bf.size(), rank=bf.rank()\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler, **kwargs\n",
    ")\n",
    "\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    val_dataset, num_replicas=bf.size(), rank=bf.rank()\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_batch_size, sampler=val_sampler, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c324644d036"
   },
   "source": [
    "## Set up standard torch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6cb62e20c52f"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(num_classes=10)\n",
    "if run_on_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Scale learning rate by the number of GPUs.\n",
    "base_lr = 0.0125\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00005\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=(base_lr * bf.size()),\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db3803bdc39d"
   },
   "source": [
    "## Wrap the torch standard optimizer into BlueFog distributed one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0016e16b8e2e"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "atc_style = False\n",
    "base_dist_optimizer = (\n",
    "    bf.DistributedAdaptThenCombineOptimizer\n",
    "    if atc_style\n",
    "    else bf.DistributedAdaptWithCombineOptimizer\n",
    ")\n",
    "optimizer = base_dist_optimizer(\n",
    "    optimizer, model=model, communication_type=bf.CommunicationType.neighbor_allreduce\n",
    ")\n",
    "\n",
    "# Bluefog: broadcast parameters & optimizer state.\n",
    "bf.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "bf.broadcast_optimizer_state(optimizer, root_rank=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f83fffb8187c"
   },
   "source": [
    "## Define several helper functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "424fe7146cc1"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).cpu().float().mean()\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "        self.n = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += bf.allreduce(val.detach().cpu(), name=self.name)\n",
    "        self.n += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "\n",
    "dynamic_neighbor_allreduce_gen = topology_util.GetDynamicOnePeerSendRecvRanks(\n",
    "    bf.load_topology(), bf.rank()\n",
    ")\n",
    "\n",
    "\n",
    "def dynamic_topology_update(epoch, batch_idx):\n",
    "    send_neighbors, recv_neighbors = next(dynamic_neighbor_allreduce_gen)\n",
    "    optimizer.send_neighbors = send_neighbors\n",
    "    optimizer.neighbor_weights = {\n",
    "        r: 1 / (len(recv_neighbors) + 1) for r in recv_neighbors\n",
    "    }\n",
    "    optimizer.self_weight = 1 / (len(recv_neighbors) + 1)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(epoch, batch_idx):\n",
    "    if epoch < 5:  # warmup_epochs\n",
    "        epoch += float(batch_idx + 1) / len(train_loader)\n",
    "        lr_adj = 1.0 / bf.size() * (epoch * (bf.size() - 1) / 5 + 1)\n",
    "    elif epoch < 30:\n",
    "        lr_adj = 1.0\n",
    "    elif epoch < 60:\n",
    "        lr_adj = 1e-1\n",
    "    elif epoch < 80:\n",
    "        lr_adj = 1e-2\n",
    "    else:\n",
    "        lr_adj = 1e-3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = base_lr * bf.size() * lr_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0e5522361fa3"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# publish_data is useful for monitoring the status of workers\n",
    "from ipyparallel.datapub import publish_data\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_loss = Metric(\"train_loss\")\n",
    "    train_accuracy = Metric(\"train_accuracy\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        adjust_learning_rate(epoch, batch_idx)\n",
    "        dynamic_topology_update(epoch, batch_idx)\n",
    "\n",
    "        if run_on_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        train_accuracy.update(accuracy(output, target))\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        train_loss.update(loss)\n",
    "        # Average gradients among sub-batches\n",
    "        loss.div_(math.ceil(float(len(data)) / batch_size))\n",
    "        loss.backward()\n",
    "        # Gradient is applied across all ranks\n",
    "        optimizer.step()\n",
    "        publish_data(\n",
    "            dict(\n",
    "                batch_idx=batch_idx,\n",
    "                loss=train_loss.avg.item(),\n",
    "                accuracy=100.0 * train_accuracy.avg.item(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = Metric(\"val_loss\")\n",
    "    val_accuracy = Metric(\"val_accuracy\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            if run_on_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "\n",
    "            val_loss.update(torch.nn.functional.cross_entropy(output, target))\n",
    "            val_accuracy.update(accuracy(output, target))\n",
    "            publish_data(\n",
    "                dict(\n",
    "                    batch_idx=batch_idx,\n",
    "                    loss=val_loss.avg.item(),\n",
    "                    accuracy=100.0 * val_accuracy.avg.item(),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fake example of monitoring the status of workers\n",
    "\n",
    "The trainning of deep learning model can be time consuming. In order to monitor the \n",
    "status of workers, you can use `ipyparallel.datapub.publish_data` to push the data\n",
    "of the workers to the notebook. Following is a simple fake example to help you understand\n",
    "how to use publish_data and [tqdm](https://tqdm.github.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# publish_data is useful for monitoring the status of workers\n",
    "import time\n",
    "from ipyparallel.datapub import publish_data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fake_training():\n",
    "    for i in range(n_iter):\n",
    "        loss, accuracy = np.random.rand(2)\n",
    "        publish_data(dict(i=i, loss=loss, accuracy=accuracy))\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch     #2: 100%|██████████| 49/49 [00:25<00:00,  1.95it/s, loss=0.846, accuracy=84.98%]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dview = rc[:]\n",
    "dview.block = True\n",
    "n_iter = 50\n",
    "dview[\"n_iter\"] = n_iter\n",
    "ar = dview.apply_async(lambda: fake_training())\n",
    "epoch = 1\n",
    "\n",
    "with tqdm(total=n_iter - 1, desc=\"Train Epoch     #{}\".format(epoch + 1)) as t:\n",
    "    while not ar.ready():\n",
    "        data = ar.data[0]\n",
    "        t.set_postfix(\n",
    "            {\n",
    "                \"loss\": data.get(\"loss\", 0),\n",
    "                \"accuracy\": f\"{data.get('accuracy', 0)*100:.2f}%\",\n",
    "            }\n",
    "        )\n",
    "        t.update(data.get(\"i\", 0) - t.n)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a16c8852affb"
   },
   "source": [
    "## Start decentralized trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch     #1: 100%|██████████| 390/390 [00:32<00:00, 12.12it/s, loss=1.72, accuracy=41.46%]\n",
      "Validate Epoch  #1: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s, loss=1.51, accuracy=52.60%]\n",
      "Train Epoch     #2: 100%|██████████| 390/390 [00:32<00:00, 12.12it/s, loss=1.23, accuracy=57.31%]\n",
      "Validate Epoch  #2: 100%|██████████| 2/2 [00:00<00:00,  2.40it/s, loss=1.75, accuracy=58.47%]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    rc[:].push({\"epoch\": epochs}, block=True)\n",
    "\n",
    "    # Training\n",
    "    with tqdm(\n",
    "        total=len(rc[0][\"train_loader\"]) - 1,\n",
    "        desc=\"Train Epoch     #{}\".format(epoch + 1),\n",
    "    ) as t:\n",
    "        ar_train = rc[:].apply_async(lambda: train(epoch))\n",
    "        while not ar_train.ready() or t.n < t.total:\n",
    "            data = ar_train.data[0]\n",
    "            t.set_postfix(\n",
    "                {\n",
    "                    \"loss\": data.get(\"loss\", 0),\n",
    "                    \"accuracy\": f\"{data.get('accuracy', 0):.2f}%\",\n",
    "                }\n",
    "            )\n",
    "            t.update(data.get(\"batch_idx\", 0) - t.n)\n",
    "            time.sleep(0.5)\n",
    "    # Validation\n",
    "    with tqdm(\n",
    "        total=len(rc[0][\"val_loader\"]) - 1, desc=\"Validate Epoch  #{}\".format(epoch + 1)\n",
    "    ) as t:\n",
    "        ar_val = rc[:].apply_async(lambda: validate(epoch))\n",
    "        while not ar_val.ready() or t.n < t.total:\n",
    "            data = ar_val.data[0]\n",
    "            t.set_postfix(\n",
    "                {\n",
    "                    \"loss\": data.get(\"loss\", 0),\n",
    "                    \"accuracy\": f\"{data.get('accuracy', 0):.2f}%\",\n",
    "                }\n",
    "            )\n",
    "            t.update(data.get(\"batch_idx\", 0) - t.n)\n",
    "            time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51577e0b594e"
   },
   "source": [
    "# Demystify the BlueFog High Level APIs\n",
    "\n",
    "Here we show more details about two common BlueFog high level APIs -- `DistributedAdaptWithCombineOptimizer` and `DistributedAdaptThenCombineOptimizer`. They are corresponding to ATC and AWC algorithm we introduced before. After understanding how these works, you should be able to quickly grasp main idea of other high level APIs or even build your own high level APIs with different decentralized algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "800ee87c54bb"
   },
   "source": [
    "From the algorithm part, there is nothing new happened under the these two distributed optimizer. But we update our notation slightly since in neural network it is common to denote the gradient layer by layer:\n",
    "\n",
    "\\begin{align}\n",
    "    grad_{l}(w_{k,i}) \n",
    "     =& \\frac{\\partial f_k(w_{k,i})}{\\partial w_{k,i}[l]} \\\\\n",
    "     =& \\frac{\\partial f_k(w_{k,i})}{\\partial w_{k,i}[l+1]}\\frac{\\partial w_{k,i}[l+1]}{\\partial w_{k,i}[l]} \\\\\n",
    "     =& grad_{l+1}(w_{k,i}) \\frac{\\partial w_{k,i}[l+1]}{\\partial w_{k,i}[l]}\n",
    "\\end{align}\n",
    "where we use $l$ for $l-$ layer and $[l]$ means the parameter related to layer $l$. In this notation, it is closer to reflect how torch works. Except that, the AWC and ATC are applied in the same way. However, in the engineering part,\n",
    "it is not the same as we did before. The main difference is that we no longer wait for the gradient computation of whole model finished to start the communication. Instead, we try to maximize the communication and computation through layer-wise style. \n",
    "\n",
    "\n",
    "First, let's review the typical deep learning training under Torch framework:\n",
    "```python\n",
    "loss = metric(model(data), target)\n",
    "loss.backward()  <--- # it starts to compute the gradient\n",
    "optimizer.step() <--- # it applies the computed gradient on the weights\n",
    "```\n",
    "The `backwards()` function will inversely traverse the neural networks and compute the gradients of each components in the model. The earliest time of executing communication is either at the *forward* or the *backward* step depending on the communication vector required gradient information or not. The latest time of finishing communication is at the `step()` step.\n",
    "A typical communication and computation timeline for Adapt-With-Combine optimizer and Adapt-Then-Combine optimizer is shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddee22c408a2"
   },
   "source": [
    "<img src=\"atc_awc_timeline.png\" alt=\"atc_awc_timeline.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0c5a5c3b8cf"
   },
   "source": [
    "To achieve that the key mechanism is `hook` pattern or so-called [Template method pattern](https://en.wikipedia.org/wiki/Template_method_pattern). For those who are not familar with this design pattern, we provide a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4032b7a550d7"
   },
   "outputs": [],
   "source": [
    "class TriviaHookPattern:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def register_pre_hook(self, f):\n",
    "        self._pre_hook = f\n",
    "\n",
    "    def register_post_hook(self, f):\n",
    "        self._post_hook = f\n",
    "\n",
    "    def execute(self):\n",
    "        self._pre_hook()\n",
    "        print(\"Execute some function\")\n",
    "        self._post_hook()\n",
    "\n",
    "    def _pre_hook(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _post_hook(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "12230f1cb516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am pre hook\n",
      "Execute some function\n",
      "I am post hook\n"
     ]
    }
   ],
   "source": [
    "h = TriviaHookPattern()\n",
    "h.register_pre_hook(lambda: print(\"I am pre hook\"))\n",
    "h.register_post_hook(lambda: print(\"I am post hook\"))\n",
    "h.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b98c5a0be558"
   },
   "source": [
    "There are three types of hooks provided by torch APIs.\n",
    "\n",
    "- `register_forward_hook` for one `torch.nn` module\n",
    "- `register_backward_hook` for one `torch.nn` module\n",
    "- `register_hook()` in gradient computation of one `torch.Tensor`\n",
    "\n",
    "We implement distributed optimizers with some proper combination of these hooks. We would not list the full source code due to too many engineering details. But we demonstrate a one-layer neural network example to illustrate how to combine it with bluefog non-blocking communication operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "44dac4674ae8"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.num_parameters = input_dim * output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fd4cf34241a5"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# register for communication hook for model\n",
    "data, target = torch.rand(5), torch.rand(1)\n",
    "model = LinearNet(5, 1)\n",
    "\n",
    "handles = []\n",
    "\n",
    "\n",
    "def make_hook(model):\n",
    "    def hook(model, *unused):\n",
    "        for name, parameter in model.named_parameters():\n",
    "            print(\"Make neighbor_allreduce for \", name)\n",
    "            handle = bf.neighbor_allreduce_nonblocking(parameter.data)\n",
    "            handles.append(handle)\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "hook = make_hook(model)\n",
    "_ = model.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "c0c5fa8a9f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n",
      "[stdout:1] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n",
      "[stdout:2] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n",
      "[stdout:3] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "# Start nonblocking communication at first time.\n",
    "# Wait until it finished at last step.\n",
    "y = model(data)\n",
    "loss = nn.MSELoss()(y, target)\n",
    "loss.backward()\n",
    "print(handles)\n",
    "tensors = [bf.wait(handle) for handle in handles]\n",
    "handles.clear()\n",
    "print(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Applying BlueFog on Deep Learning problem(High Level API Introduction).ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
